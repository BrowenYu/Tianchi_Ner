

NER博客和文章：
	中文NER碎碎念―聊聊词汇增强与实体嵌套 https://blog.csdn.net/u011983997/article/details/122877003
	JayJay 公众号《高能AI》https://www.zhihu.com/people/lou-jie-9/posts?page=1
	paperweekly https://so.csdn.net/so/search?q=NER&t=blog&u=c9Yv2cf9I06K2A9E&s=new
	刘东泽 深度学习小学生 https://www.zhihu.com/people/liudongze-50/posts
	NER解码之GlobalPointer https://zhuanlan.zhihu.com/p/464361607
	NER的过去、现在和未来综述-现在 https://zhuanlan.zhihu.com/p/425268651
	NER中的一些编码器与解码器：https://blog.csdn.net/init__/article/details/121405107
	中文NER任务实验小结：BERT-MRC的再优化 https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/122227248?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0-122227248-blog-111087735.pc_relevant_antiscanv2&spm=1001.2101.3001.4242.1&utm_relevant_index=3
	LEBERT：基于词汇增强的中文NER模型 https://kaiyuan.blog.csdn.net/article/details/124113979?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-124113979-blog-111087735.pc_relevant_antiscanv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-124113979-blog-111087735.pc_relevant_antiscanv2&utm_relevant_index=5
	简单有效，来看看这个NER SOTA！ https://blog.csdn.net/Kaiyuan_sjtu/article/details/124507360?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165323317216781683965266%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=165323317216781683965266&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-2-124507360-null-null.article_score_rank_blog&utm_term=NER&spm=1018.2226.3001.4450
	绝了！关系抽取新SOTA https://blog.csdn.net/Kaiyuan_sjtu/article/details/124418474?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165323317216781683965266%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=165323317216781683965266&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-3-124418474-null-null.article_score_rank_blog&utm_term=NER&spm=1018.2226.3001.4450
	ACL2021 | 信息抽取相关论文 https://blog.csdn.net/Kaiyuan_sjtu/article/details/118836808?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165323317216781683965266%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=165323317216781683965266&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-4-118836808-null-null.article_score_rank_blog&utm_term=NER&spm=1018.2226.3001.4450
	中文NER的正确打开方式: 词汇增强方法总结 (从Lattice LSTM到FLAT) https://blog.csdn.net/qq_27590277/article/details/111087735?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-4.pc_relevant_default&spm=1001.2101.3001.4242.3&utm_relevant_index=7
	

	
	
NER论文：
	A Survey on Deep Learning for Named Entity Recognition https://arxiv.org/pdf/1812.09449.pdf
	Improving Chinese Word Segmentation with Wordhood Memory Networks
	Joint Chinese Word Segmentation and Part-of-speech Tagging via Multi-channel Attention of Character N-grams

 	

NERforQA论文：
	QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition https://arxiv.org/pdf/2203.01543.pdf
	NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension https://arxiv.org/pdf/2205.05904.pdf
	

MRC相关：Retrospective Reader for Machine Reading Comprehension https://arxiv.org/pdf/2001.09694v4.pdf
	Towards More Equitable Question Answering Systems:How Much More Data Do You Need? https://arxiv.org/pdf/2105.14115.pdf
	Unifying Question Answering, Text Classification, and Regression via Span Extraction https://arxiv.org/pdf/1904.09286.pdf
	机器阅读理解（MRC）必看论文 - BiDAF https://zhuanlan.zhihu.com/p/471435094
        【论文笔记】A Unified MRC Framework for Named Entity Recognition https://blog.csdn.net/yuexiaomao/article/details/108973018